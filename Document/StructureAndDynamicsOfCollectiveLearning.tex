\documentclass[hidelinks,preprint,12pt]{elsarticle}
\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of ...}

%\usepackage{natbib}
%\bibliographystyle{humannat}
\bibliographystyle{elsarticle-harv}\biboptions{authoryear}


%====================================================
% MY PACKAGES
%------------------------
\usepackage{amsmath,amsfonts,amsthm,array,amscd,amssymb}
\usepackage{color}
\usepackage{cleveref}%to have automatic 'clever' references that includes the words 'figure', 'equation', etc.
\usepackage{hyperref}
\usepackage{dcolumn} % for stargazer tables from R
%for subfigures with their own captions
\usepackage[font={footnotesize}]{caption}
\usepackage{subcaption}
\usepackage{makecell} %to add additional line breaks inside cell in table
%\renewcommand{\cellalign/theadalign}{tl}
%\renewcommand\theadalign{tl}
\usepackage{rotating}

% To change the position of page number and text:
\setlength{\footskip}{35pt}

%====================================================
% MY COMMANDS
%------------------------
\newcommand{\ud}{\mathrm{d}}
\newcommand{\e}{\text{e}}
\newcommand{\mtx}[1]{\mathbf{ #1}}
\newcommand{\tr}[1]{\mathrm{tr\left[\mtx{ #1}\right]}}
\newcommand{\rank}[1]{\mathrm{rank\left[\mtx{ #1}\right]}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\EE}[1]{\mathrm{E\left[\mtx{ #1}\right]}}
\newcommand{\E}[1]{{\mathrm E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}

\newcommand\given[1][]{\:#1\vert\:}
%Which will be manually scalled via, say
%\given[\Big]

% from https://tex.stackexchange.com/questions/116675/extra-appendix-when-referencing-using-cleveref-in-elsarticle
%\def\appendixname{}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT
\begin{document}


\begin{frontmatter}

\title{Linking Structure to the Dynamics of Collective Learning Through Diffusion Maps\tnoteref{titlenote1}}
\tnotetext[titlenote1]{Working paper. Do not share.}

%% Group authors per affiliation:
\author[hks]{Andr\'{e}s G\'{o}mez-Li\'{e}vano\corref{correspauthor1}}
\ead{Andres\_Gomez@hks.harvard.edu}
\author[hks]{Michele Coscia}
\ead{Michele\_Coscia@hks.harvard.edu}
\author[hks]{Frank Neffke}
\ead{Frank\_Neffke@hks.harvard.edu}
\author[hks]{Ricardo Hausmann}
\ead{Ricardo\_Hausmann@hks.harvard.edu}

\cortext[correspauthor1]{Corresponding author}

\address[hks]{Center for International Development, Harvard University, Cambridge, USA}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
\begin{abstract}
Collective know-how accumulates as capabilities diffuse from group to group, and from generation to generation. 
Lack of standard formalisms to study how this accumulation occurs precludes the development of a unified view of human development that is both insightful and predictive. 
We present one such formalism based on Diffusion Maps, which provide a low dimensional representation of the complex process of collective learning. 
We empirically find that collective learning depends on the amount of collective know-how, but also on the relative position in knowledge space with respect to other populations. 
The latter is important because it determines the available knowledge that can be transferred from one society to another. 
Hence, these low-dimensional spaces contain metrics that can be used to quantify how much societies know, and predict future paths of development. 
As a case study we consider data on trade, which provides consistent data on the technological diversification of hundreds of countries across more than 50 years. 
Our results are relevant for anthropologists, sociologists, and economists interested in the role of collective know-how as the main determinant of the success and welfare of a society.
\end{abstract}

\begin{keyword}
collective learning\sep economic complexity\sep cultural evolution\sep economic development
\end{keyword}

\end{frontmatter}

\linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notes on literature}

\begin{itemize}
\item \citet{coifman_diffusion_2006} \\ 
P: random graphs have proven to be very efficient at finding relevant structures in complex geometries. \\
P: the field of application [of eigenvectors of markov chains] then goes beyond the ideas of clustering and ranking, as using multiple eigenvectors allows to speak of \emph{parametrization} of data sets. A great deal of attention has been recently paid to the so-called ``kernel eigenmap methods'' such as local linear
embedding [21], Laplacian eigenmaps [11], Hessian eigenmaps [18] and local tangent space alignment [25]. The
remarkable idea emerging from these papers is that eigenvectors of Markov matrices can be thought of as coordinates
on the data set. Therefore, the data, originally modeled as a graph, can be represented (embedded) as a cloud of points
in a Euclidean space. \\
P: These algorithms exhibit two major advantages
over classical dimensionality reduction methods (such as principal component analysis or classical multidimensional
scaling): they are nonlinear, and they preserve local structures. \\
P: The approach that we present generalizes the classical Newtonian paradigm in which
local infinitesimal rules of transition of a system lead to global macroscopic descriptions by integration. \\
M: Since development occurs very slowly [citation?], and the process of diversification/trait accumulation is a structured process [ProductSpace], we need a representation of the data that preserves the local structures and the local geometry. \\




\item \citet{lafon_diffusion_2006}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}\label{sec:Background}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analytic Results}\label{sec:analytic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Elasticity for a single city}
\label{ch:betasingle}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulations}\label{sec:simulation}



%\begin{figure}[!t]
	%\centering
		%\includegraphics[width=0.31\textwidth]{C:/Users/agomez/Dropbox/ASU/Projects/PerCapitaScaling/Figures/PerCapSca_601_severalsamples.png}
		%\includegraphics[width=0.33\textwidth]{C:/Users/agomez/Dropbox/ASU/Projects/PerCapitaScaling/Figures/PerCapSca_601_severalworkersamples_dotswitherrorbars.png}
		%\includegraphics[width=0.33\textwidth]{C:/Users/agomez/Dropbox/ASU/Projects/PerCapitaScaling/Figures/PerCapSca_601_pval_realrand_dotswitherrorbars.png}
	%\caption{
	%We analyze differently sized samples taken from all formal workers in Colombia, randomly sampled according to six different percentages. For each of the random samples, which we take ten of them, we compute the true elasticity of average wage per municipality with respect to the municipality size of total employment, and in addition, we generate a distribution of elasticities after spatially randomizing the workers. \textbf{The left panel} plots the density of the distribution elasticities calculated after all $10\times1,000$ randomizations, per sample percentage. As the sample decreases, the density of elasticities shifts to the right, but also increases its variance. \textbf{The middle panel} plots the information of the same distributions but collapsed to their mean, represented by the red circles, and their standard deviation of the distribution, represented by the segments. The blue circles represent the point estimate of the true elasticities (when workers are not randomized geographically). \textbf{The right panel} plots the $p$-values representing the probability of observing such a difference (or larger) between real and randomized elasticities, assuming the mean of the randomized elasticities is equal to the real elasticity.
	%}
%\label{fig:betas}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusions}\label{sec:Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%\input{percapita-appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
The authors gratefully acknowledge useful comments from %F. Neffke, L.M.A. Bettencourt, R. Muneepeerakul, and F. Yu.


\section*{References}

%\bibliography{C:/Users/agomez/Dropbox/Harvard/LittleProjects/StochasticPS/Document/Ref}
\bibliography{Ref,Refs20180703}

\end{document}




