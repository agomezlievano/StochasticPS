%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\RequirePackage[l2tabu, orthodox]{nag}
% this package doesnâ€™t do anything as long as your syntax is right. Load the package in the first lines of your preamble (even before the \documentclass command). It then checks for obsolete LaTeX packages and outdated commands. - See more at: http://www.howtotex.com/packages/9-essential-latex-packages-everyone-should-use/#sthash.z7Cu1RGM.dpuf
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[letterpaper, 14pt]{extarticle}




% ------
% Fonts and typesetting settings
%\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
%\usepackage{microtype}


% ------
% Page layout
%\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry}
\usepackage[margin=1.0in]{geometry}
\usepackage[font=it]{caption}
\usepackage{paralist}
%\usepackage{multicol}

% ------
% Lettrines
%\usepackage{lettrine}


% ------
% Abstract
\usepackage{abstract}
	\renewcommand{\abstractnamefont}{\normalfont\bfseries}
	\renewcommand{\abstracttextfont}{\normalfont\small\itshape}


% ------
% Titling (section/subsection)
\usepackage{titlesec}
\renewcommand\thesection{\Roman{section}}
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{}


% ------
% Header/footer
\usepackage{fancyhdr}
	\pagestyle{fancy}
	\fancyhead{}
	\fancyfoot{}
	\fancyhead[C]{Journal ZZZ $\bullet$ March 2016 $\bullet$ Vol. ZZZ, No. ZZZ}
	%\fancyfoot[RO,LE]{\thepage}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ------
% Andres' packages
\usepackage{graphicx}				% images
\usepackage{amssymb,amsmath,amsfonts,amsthm,array,amscd,amsbsy}		% math
\usepackage{bm}
\usepackage{cite}
\usepackage{float}%so that the figure does not float (problems with multicol)
\usepackage{cleveref}%to have automatic 'clever' references that includes the words 'figure', 'equation', etc.

\linespread{1.25} % Palatino needs more space between lines

% ------
% Clickable URLs (optional)
\usepackage{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\diff}{\mathrm{d}}
\newcommand{\e}{\text{e}}

\newcommand{\prob}[1]{\mathbb{P}\left [ #1 \right ]}
\newcommand{\probSymbol}{\mathbb{P}}
\newcommand{\indicator}[1]{\mathbbm{1}_{#1}}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{\mathrm{#1}}}
\newcommand{\fitted}[1]{\widehat{#1}}
\newcommand{\E}[1]{{\mathrm E}\left[ #1 \right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\transp}{{\scalebox{.8}{$\intercal$}}}
%\DeclareMathOperator*{\E}{\mathbb{E}}
%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\arcsinh}{arcsinh}
%\DeclareMathOperator*{\pseudolog}{pseudolog}
%\DeclareMathOperator*{\sign}{sign}
%\newcommand{\logoneplus}[1]{\log \left ( 1 + #1 \right )}

\newcommand\given[1][]{\:#1\vert\:}
%Which will be manually called as
%\given[\Big] 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ------
% Maketitle metadata
\title{\vspace{-15mm}%
	\fontsize{24pt}{10pt}\selectfont
	\textbf{Economic Complexity: Essentials of the properties of random walks on bipartite networks}
	}	
\author{%
	\large
	\textsc{Andres Gomez-Lievano}\thanks{\href{http://agomezlievano.weebly.com}{agomezlievano.weebly.com}} \\[2mm]
	\normalsize	Harvard University, Cambridge, MA \\
	\normalsize	\href{mailto:andres\_gomez@hks.harvard.edu}{andres\_gomez@hks.harvard.edu}
	\vspace{-5mm}
	}
\date{}



%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
\noindent An elementary discussion on the properties of the matrix that represents which products are exported by which countries is given from the formalism of simple stochastic processes. Methods to understand and manipulate the contents of this matrix can be useful for general economic analysis, and have already been used to predict of how regions develop in time. Here I present some of the mathematical characteristics of this matrix. 
\end{abstract}
	
%\lettrine[nindent=0em,lines=2]{N}{}etwork analyses applied to the process of economic developed have revealed important facts about why some countries are poor and others rich. 
Network analyses applied to the process of economic developed have revealed important facts about why some countries are poor and others rich. 

This approach has provided a formalization for the theory of economic complexity [cite Brian Arthur as well].

The stylized facts are...

The following questions still remain...

\section{Background}
We will start briefly reviewing some results from the theory of stochastic processes. 

One starts with enumerating (in the case of a discrete stochastic process) the states in which an entity can be at different time steps. After that, one establishes the probabilistic rules to transition across these states. And given this setup, there are a number of mathematical results that can be used to understand properties about the behavior of populations of ``walkers'' that randomly jump between states. The simplest of such processes is a Markov chain, in which the probability of transitioning to a new state at any given moment only depends on the current state (thus, the ``chain''). 

Let us assume states can be enumerated, and are finite, $S=1,\ldots,N$. One can think of $S$ as a random variable, that takes a particular value (e.g., $S(t)=n$) at any given time step $t$. Hence, a (discrete-time) Markov chain is one in which
$$
	\Pr\{S(t+1)|S(t),S(t-1),\ldots,S(t_0)\}=\Pr\{S(t+1)|S(t)\},
$$
where $\Pr\{S(t+1)|S(t)\}$ is the \emph{conditional} probability of transitioning to state $S(t+1)$ given the current state is $S(t)$.

The law of total probabilities states that a state can be arrived at by different means. In probability terms, it is just a relation between a joint distribution and a marginal distribution. In our context, we can write the marginal probability of being at state $n$ at time step $t+1$ as
\begin{eqnarray}
	\Pr\{S(t+1)=n\}&=&\sum_{m=1}^N \Pr\{S(t+1)=n,S(t)=m\} \nonumber \\
	&=&\sum_{m=1}^N \Pr\{S(t+1)=n|S(t)=m\}\Pr\{S(t)=m\}. \label{eq:onestep}
\end{eqnarray}

Note that this can be iterated, since the state at $t$ has also a probability, which can be computed similarly, $\Pr\{S(t)=m\}=\sum_{k=1}^N \Pr\{S(t)=m|S(t-1)=k\}\Pr\{S(t-1)=k\}$. Thus,
\begin{eqnarray}
	\Pr\{S(t+1)=n\}&=&\sum_{m=1}^N \sum_{k=1}^N \Pr\{S(t+1)=n|S(t)=m\}\Pr\{S(t)=m|S(t-1)=k\}\Pr\{S(t-1)=k\}.\label{eq:twosteps}
\end{eqnarray}

It is worth remembering that probabilities have to add up to one. Hence, $\sum_{n=1}^N \Pr\{S(t+1)=n|S(t)=m\}=1$ (note the index of the sum refers to future states), and $\sum_{n=1}^N \Pr\{S(t+1)=n\}=1$.

Markov chains are often better understood using matrices, such that $[\mat{P}]_{n,m}=\Pr\{S(t+1)=n|S(t)=m\}$. $\mat{P}$ is called the \emph{transition matrix} and has size $N\times N$. Here, I will use the convention that columns of the matrix represent the current state, and rows represent the future state. Hence,
\begin{equation}
	\mat{P}=
	\left(
	\begin{array}{ccc}
		\Pr\{S(t+1)=1|S(t)=1\} & \cdots & \Pr\{S(t+1)=1|S(t)=N\} \\
		\vdots & \ddots & \vdots \\
		\Pr\{S(t+1)=N|S(t)=1\} & \cdots & \Pr\{S(t+1)=N|S(t)=N\}
	\end{array}
	\right).
\end{equation}
We will assume that this matrix is constant, and does not change in time.

\subsection{Multiplying on the right or on the left}
The properties of matrix $\mat{P}$ are such that
\begin{itemize}
	\item multiplying it by a (column) vector on the right \emph{propagates probabilities}, and
	\item multiplying it by a (row) vector on the left \emph{takes averages} across the nodes.
\end{itemize}

If $\vec{p}(t)$ is a column vector, such that the $n$-th element is the probability at time $t$ of being in state $n$, $p_n(t)=\Pr\{S(t+1)=n\}$, then \cref{eq:onestep,eq:twosteps} can be represented as $\vec{p}(t+1)=\mat{P}\vec{p}(t)$ and $\vec{p}(t+1)=\mat{P}^2\vec{p}(t-1)$, respectively. In general,
$$
	\vec{p}(t_0+s)=\mat{P}^s\vec{p}(t_0).
$$

On the other hand, if $\vec{x_t}^\transp$ is a vector of properties across nodes, then multiplying this vector on the left of $\mat{P}$ generates another vector $\vec{x_{t+1}}^\transp$ in which the element $[\vec{x_{t+1}}]_m$ represents the expected value of the properties in $\vec{x}^\transp$ if the random walker had started at node $m$. In particular, if $\vec{x_t}^\transp$ is a vector of 1's, then the result is also a vector of 1's, and this simply reflects the fact that $\mat{P}$ is column-normalized.

\Cref{tab:simpleprops} shows these properties as matrix operations or in conventional probabilistic terms.
\begin{table}
\footnotesize
\centering
	\begin{tabular}{ll}
		\hline
		\textbf{Conventional Representation} & \textbf{Matrix Notation} \\
		\hline 
		$\sum_{n=1}^N \Pr\{S(t+1)=n\}=1$ & $\vec{1}^\transp\vec{p}(t)$ \\
		$\sum_{n=1}^N \Pr\{S(t+1)=n|S(t)=m\}=1$ & $\vec{1}^\transp\mat{P}$ \\
		$\Pr\{S(t+1)=n\}=\sum_{m=1}^N \Pr\{S(t+1)=n|S(t)=m\}\Pr\{S(t)=m\}$ & $\vec{p}(t+1)=\mat{P}\vec{p}(t)$ \\
		\hline 
	\end{tabular}
\caption{Matrix representation of Markov Chain properties.}
\label{tab:simpleprops}
\end{table}

\subsection{Network interpretation of a Markov Chain}
The states in a Markov Chain can be represented as nodes in a network. Two edges can exist between pairs of nodes, representing the transition probabilities between both states. The Markov Chain is thus a random walk over this network of states, and $\vec{p}(t+1)$ is the probability of finding a random walker in the different nodes at a given point in time.
 
The Perron-Frobenius theorem is a result that states that under certain general conditions regarding the network of states, the matrix $\mat{P}$ represents a process in which the probability of finding the random walker in any given node $n$ converges to a unique (non-negative) value [cite] as time progresses. When these conditions are met,
$$
\vec{p}_\infty=\lim_{t\rightarrow\infty}\mathrm{P}^t\vec{p}(t_0),
$$
where $\vec{p}_\infty$ is called the perron vector of probabilities, and represents the stationary distribution of the markov chain. The convergence to a unique vector of probabilities implies that $\vec{p}_\infty = \mat{P}\vec{p}_\infty$, which means that the stationary distribution is the \emph{right}-eigenvector of matrix $\mat{P}$. One of the consequences of the Perron-Frobenius Theorem is that this eigenvector is, in fact, associated with the \emph{dominant} eigenvalue $\lambda_1=1$.






%
%
%\begin{figure*}%[H]
		%\centering
			%\includegraphics[width=0.6\linewidth]{figures/WagesCCDF_AllColombia.png}
		%\caption{Cumulative distribution of monthly incomes in Colombia, for a total of 11,548,586 observations. Source: PILA for the year 2013.}
%\label{fig:distColombia}
%\end{figure*}




\bibliographystyle{phcpc}%{unsrt}%{apalike}%cell, phiaea, nature, plain, phcpc, unsrt, amsref, apalike
\bibliography{C:/Users/agomez/Dropbox/Harvard/LittleProjects/StochasticPS/StochasticPS}


\end{document}
